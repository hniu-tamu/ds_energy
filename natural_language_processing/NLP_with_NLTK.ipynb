{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/hniu-tamu/e808c2882a8f942bb879504fb3324771/nlp_with_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMYbTJGjtMoz"
      },
      "source": [
        "# Natural Language Processing with NLTK\n",
        "\n",
        "Haoyu Niu, Texas A&M University\n",
        "\n",
        "Feb 18, 2025\n",
        "\n",
        "Converted from\n",
        "\n",
        "**Intro to natural language processing with Python**\n",
        "\n",
        "Notebook by [Juan Cruz Martinez](https://livecodestream.dev/authors/bajcmartinez/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfKt9ernTGcK"
      },
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5aFgDKqOBI"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4JpHHRXTKrI"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PwgHiH5qTQs"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZAtuBOHr0Hi"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "Text = \"Good morning, How you doing? Are you coming tonight?\"\n",
        "Tokenized = word_tokenize(Text)\n",
        "print(Tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZavEyf3shdA"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "Text = \"Good morning, How you doing? Are you coming tonight?\"\n",
        "Tokenized = sent_tokenize(Text)\n",
        "print(Tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug23r1ziTRBB"
      },
      "source": [
        "## Stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J_X7WZCtvSO"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkFPhwmAt0_m"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "Text = [\"Good\", \"morning\", \"How\", \"you\", \"doing\", \"Are\", \"you\", \"coming\", \"tonight\"]\n",
        "for i in Text:\n",
        "   if i not in stopwords:\n",
        "       print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbC3CKm-vTHs"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "','.join(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPDmzpqRTUdn"
      },
      "source": [
        "## Stemming Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI2Olf5asdTY"
      },
      "source": [
        "help(nltk.stem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st2JbM81u0RJ"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = [\"Loving\", \"Chocolate\", \"Retrieving\"]\n",
        "for i in words:\n",
        "   print(ps.stem(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBjGg2nrTYAm"
      },
      "source": [
        "## Counting Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WchynDMVnu8"
      },
      "source": [
        "import nltk\n",
        "words = [\"men\", \"teacher\", \"men\", \"woman\"]\n",
        "FreqDist = nltk.FreqDist(words)\n",
        "for i,j in FreqDist.items():\n",
        "   print(i, \"---\", j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6au6fb1Tilk"
      },
      "source": [
        "## Word groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eWR25NzQDtv"
      },
      "source": [
        "words = \"Learning python was such an amazing experience for me\"\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.bigrams(word_tokenize)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzrbKs2jRFfP"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.trigrams(word_tokenize)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj1BCGX_RX_J"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.ngrams(word_tokenize, 4)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7BwByxESK4n"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3mvsterSH2a"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "LKWOzz6ay2z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZiPb5cPSRAG"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "Lem = WordNetLemmatizer()\n",
        "print(Lem.lemmatize(\"He believes me\"))\n",
        "print(Lem.lemmatize(\"retrieved\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgt9YaYISxMv"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "Lem = WordNetLemmatizer()\n",
        "print(Lem.lemmatize(\"believes\", pos=\"v\"))\n",
        "print(Lem.lemmatize(\"believes\", pos=\"n\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4TVOQOBEnX6I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd30v8wvS8cM"
      },
      "source": [
        "## POS Taggers\n",
        "Exampls\n",
        "* PRP\tpersonal pronoun (hers, herself, him, himself)\n",
        "* RB\tadverb (occasionally, swiftly)\n",
        "* VBP\tverb, present tense not 3rd person singular(wrap)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Z59DxgTCaD"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "words = \"we work here\"\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(nltk.pos_tag(word_tokenize))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vGnMzBTVAn6"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hAhTQJaU--_"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1NpKVSHWSfj"
      },
      "source": [
        "Text = \"tom is in london\"\n",
        "Tokenize = nltk.word_tokenize(Text)\n",
        "POS_tags = nltk.pos_tag(Tokenize)\n",
        "NameEn = nltk.ne_chunk(POS_tags)\n",
        "print(NameEn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekKMimU1YZ1B"
      },
      "source": [
        "!pip3 install textblob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5DGtIvNYffo"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Joe_Biden_Tweet = \"today is great!\"\n",
        "Joe_Biden = TextBlob(Joe_Biden_Tweet)\n",
        "print(Joe_Biden.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG2gqbhMZqP8"
      },
      "source": [
        "## Spelling Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9QWtSAyZqvO"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Text = \"Smalle businesses neede relief\"\n",
        "spelling_mistakes = TextBlob(Text)\n",
        "print(spelling_mistakes.correct())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
